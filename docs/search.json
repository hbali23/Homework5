[
  {
    "objectID": "Homework 5.html",
    "href": "Homework 5.html",
    "title": "Homework 5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model? Cross-validation helps assess the performance of a random forest model by iteratively splitting the data into training and validation sets, ensuring robustness in model evaluation and hyperparameter tuning.\nDescribe the bagged tree algorithm. Bagging (Bootstrap Aggregating) involves creating multiple bootstrap samples from the original dataset, training a decision tree on each sample, and then aggregating their predictions (e.g., averaging for regression or voting for classification) to reduce variance and improve model accuracy.\nWhat is meant by a general linear model? A general linear model (GLM) is a statistical model that assumes a linear relationship between the dependent variable and predictor variables, with normally distributed errors.\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model? Adding an interaction term allows the model to capture non-additive effects between predictors, meaning the relationship between the dependent variable and one predictor variable may vary depending on the value of another predictor.\nWhy do we split our data into a training and test set? Splitting data into training and test sets helps evaluate the performance of a predictive model on unseen data, thereby estimating how well the model generalizes to new, unseen observations and avoiding overfitting to the training data."
  },
  {
    "objectID": "Homework 5.html#task-1-conceptual-questions",
    "href": "Homework 5.html#task-1-conceptual-questions",
    "title": "Homework 5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model? Cross-validation helps assess the performance of a random forest model by iteratively splitting the data into training and validation sets, ensuring robustness in model evaluation and hyperparameter tuning.\nDescribe the bagged tree algorithm. Bagging (Bootstrap Aggregating) involves creating multiple bootstrap samples from the original dataset, training a decision tree on each sample, and then aggregating their predictions (e.g., averaging for regression or voting for classification) to reduce variance and improve model accuracy.\nWhat is meant by a general linear model? A general linear model (GLM) is a statistical model that assumes a linear relationship between the dependent variable and predictor variables, with normally distributed errors.\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model? Adding an interaction term allows the model to capture non-additive effects between predictors, meaning the relationship between the dependent variable and one predictor variable may vary depending on the value of another predictor.\nWhy do we split our data into a training and test set? Splitting data into training and test sets helps evaluate the performance of a predictive model on unseen data, thereby estimating how well the model generalizes to new, unseen observations and avoiding overfitting to the training data."
  },
  {
    "objectID": "Homework 5.html#task-2",
    "href": "Homework 5.html#task-2",
    "title": "Homework 5",
    "section": "Task 2",
    "text": "Task 2\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(tidyr)"
  },
  {
    "objectID": "Homework 5.html#quickly-understand-your-data.-check-on-missingness-and-summarize-the-data-especially-with-respect",
    "href": "Homework 5.html#quickly-understand-your-data.-check-on-missingness-and-summarize-the-data-especially-with-respect",
    "title": "Homework 5",
    "section": "1. Quickly understand your data. Check on missingness and summarize the data, especially with respect",
    "text": "1. Quickly understand your data. Check on missingness and summarize the data, especially with respect\nto the relationships of the variables to HeartDisease.\n\n#Read in data\nheart &lt;- read_excel(\"heart.xls\")\n\n#Understand the structure of the dataset\nstr(heart)\n\ntibble [918 × 12] (S3: tbl_df/tbl/data.frame)\n $ Age           : num [1:918] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr [1:918] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr [1:918] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : num [1:918] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : num [1:918] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : num [1:918] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr [1:918] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : num [1:918] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr [1:918] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num [1:918] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope      : chr [1:918] \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease  : num [1:918] 0 1 0 1 0 0 0 0 1 0 ...\n\nsummary(heart)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n#Check for missing values\nmissing_values &lt;- sapply(heart, function(x) sum(is.na(x)))\nprint(missing_values)\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n# Summary statistics for numeric variables\nnumeric_summary &lt;- heart %&gt;% \n  select_if(is.numeric) %&gt;%\n  summary()\nprint(numeric_summary)\n\n      Age          RestingBP      Cholesterol      FastingBS     \n Min.   :28.00   Min.   :  0.0   Min.   :  0.0   Min.   :0.0000  \n 1st Qu.:47.00   1st Qu.:120.0   1st Qu.:173.2   1st Qu.:0.0000  \n Median :54.00   Median :130.0   Median :223.0   Median :0.0000  \n Mean   :53.51   Mean   :132.4   Mean   :198.8   Mean   :0.2331  \n 3rd Qu.:60.00   3rd Qu.:140.0   3rd Qu.:267.0   3rd Qu.:0.0000  \n Max.   :77.00   Max.   :200.0   Max.   :603.0   Max.   :1.0000  \n     MaxHR          Oldpeak         HeartDisease   \n Min.   : 60.0   Min.   :-2.6000   Min.   :0.0000  \n 1st Qu.:120.0   1st Qu.: 0.0000   1st Qu.:0.0000  \n Median :138.0   Median : 0.6000   Median :1.0000  \n Mean   :136.8   Mean   : 0.8874   Mean   :0.5534  \n 3rd Qu.:156.0   3rd Qu.: 1.5000   3rd Qu.:1.0000  \n Max.   :202.0   Max.   : 6.2000   Max.   :1.0000  \n\n# Count of HeartDisease cases\nheart_disease_counts &lt;- heart %&gt;% \n  group_by(HeartDisease) %&gt;% \n  summarise(count = n())\nprint(heart_disease_counts)\n\n# A tibble: 2 × 2\n  HeartDisease count\n         &lt;dbl&gt; &lt;int&gt;\n1            0   410\n2            1   508\n\n# Correlation matrix for numeric predictors\nnumeric_vars &lt;- heart %&gt;% select_if(is.numeric)\ncor_matrix &lt;- cor(numeric_vars, use = \"complete.obs\")\nprint(cor_matrix)\n\n                     Age   RestingBP Cholesterol   FastingBS      MaxHR\nAge           1.00000000  0.25439936 -0.09528177  0.19803907 -0.3820447\nRestingBP     0.25439936  1.00000000  0.10089294  0.07019334 -0.1121350\nCholesterol  -0.09528177  0.10089294  1.00000000 -0.26097433  0.2357924\nFastingBS     0.19803907  0.07019334 -0.26097433  1.00000000 -0.1314385\nMaxHR        -0.38204468 -0.11213500  0.23579240 -0.13143849  1.0000000\nOldpeak       0.25861154  0.16480304  0.05014811  0.05269786 -0.1606906\nHeartDisease  0.28203851  0.10758898 -0.23274064  0.26729119 -0.4004208\n                 Oldpeak HeartDisease\nAge           0.25861154    0.2820385\nRestingBP     0.16480304    0.1075890\nCholesterol   0.05014811   -0.2327406\nFastingBS     0.05269786    0.2672912\nMaxHR        -0.16069055   -0.4004208\nOldpeak       1.00000000    0.4039507\nHeartDisease  0.40395072    1.0000000\n\n# Strong correlations with HeartDisease indicate potential significance so age, restingbp, fastingbs and oldpeak are strongest indicators"
  },
  {
    "objectID": "Homework 5.html#create-a-new-variable-that-is-a-factor-version-of-the-heartdisease-variable-if-needed-this-depends-on",
    "href": "Homework 5.html#create-a-new-variable-that-is-a-factor-version-of-the-heartdisease-variable-if-needed-this-depends-on",
    "title": "Homework 5",
    "section": "2. Create a new variable that is a factor version of the HeartDisease variable (if needed, this depends on",
    "text": "2. Create a new variable that is a factor version of the HeartDisease variable (if needed, this depends on\nhow you read in your data). Remove the ST_Slope variable and the original HeartDisease variable (if applicable).\n\n#Create a factor version of the HeartDisease variable\nheart &lt;- heart %&gt;%\n  mutate(HeartDiseaseFactor = as.factor(HeartDisease))\n\n#Remove the ST_Slope variable and the original HeartDisease variable\nheart &lt;- heart %&gt;%\n  select(-ST_Slope, -HeartDisease)"
  },
  {
    "objectID": "Homework 5.html#well-be-doing-a-knn-model-below-to-predict-whether-or-not-someone-has-heart-disease.-to-use",
    "href": "Homework 5.html#well-be-doing-a-knn-model-below-to-predict-whether-or-not-someone-has-heart-disease.-to-use",
    "title": "Homework 5",
    "section": "3. We’ll be doing a kNN model below to predict whether or not someone has heart disease. To use",
    "text": "3. We’ll be doing a kNN model below to predict whether or not someone has heart disease. To use\nkNN we generally want to have all numeric predictors (although we could try to create our own loss function as an alternative). In this case we have some categorical predictors still in our data set: Sex, ExerciseAngina ChestPainType, and RestingECG.\nCreate dummy columns corresponding to the values of these three variables for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.\n\n# Create dummy variables for categorical predictors\ndummies &lt;- dummyVars(\"~ Sex + ExerciseAngina + ChestPainType + RestingECG\", data = heart)\ndummy &lt;- predict(dummies, newdata = heart)\n\n# Convert the dummy data to a data frame\ndummy &lt;- as.data.frame(dummy)\n\n# Add the dummy variables to the dataset\nheart &lt;- cbind(heart, dummy)\n\n# Remove the original categorical predictors\nheart &lt;- heart %&gt;%\n  select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)"
  },
  {
    "objectID": "Homework 5.html#split-your-data",
    "href": "Homework 5.html#split-your-data",
    "title": "Homework 5",
    "section": "Split your Data",
    "text": "Split your Data\n\n# Split the data into training and testing sets\nset.seed(123)  # Setting seed for reproducibility\ntrainIndex &lt;- createDataPartition(heart$HeartDiseaseFactor, p = 0.8, \n                                  list = FALSE, \n                                  times = 1)\nheart_train &lt;- heart[trainIndex,]\nheart_test  &lt;- heart[-trainIndex,]\n\n# Verify the split\ncat(\"Training set dimensions:\", dim(heart_train), \"\\n\")\n\nTraining set dimensions: 735 18 \n\ncat(\"Test set dimensions:\", dim(heart_test), \"\\n\")\n\nTest set dimensions: 183 18"
  },
  {
    "objectID": "Homework 5.html#knn-model",
    "href": "Homework 5.html#knn-model",
    "title": "Homework 5",
    "section": "KNN Model",
    "text": "KNN Model\n\n# Set up the train control for cross-validation\ntrain_control &lt;- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 3)\n\n# Set up the tuning grid for k values\ntune_grid &lt;- expand.grid(k = 1:40)\n\n# Step 9: Train the kNN model\nknn_model &lt;- train(HeartDiseaseFactor ~ ., \n                   data = heart_train, \n                   method = \"knn\", \n                   trControl = train_control, \n                   tuneGrid = tune_grid, \n                   preProcess = c(\"center\", \"scale\"))  #preprocessing data here\n\n# Print model summary\nprint(knn_model)\n\nk-Nearest Neighbors \n\n735 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 661, 663, 661, 661, 661, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7818882  0.5608002\n   2  0.7722601  0.5419087\n   3  0.8005115  0.5988926\n   4  0.7946995  0.5880570\n   5  0.8113350  0.6205271\n   6  0.8095270  0.6173046\n   7  0.8163708  0.6307341\n   8  0.8159144  0.6295971\n   9  0.8136308  0.6256473\n  10  0.8059050  0.6099189\n  11  0.8086263  0.6150179\n  12  0.8099961  0.6179845\n  13  0.8136434  0.6252583\n  14  0.8073123  0.6118871\n  15  0.8117918  0.6211873\n  16  0.8045412  0.6066372\n  17  0.8104534  0.6190159\n  18  0.8104222  0.6190393\n  19  0.8086328  0.6154342\n  20  0.8176666  0.6337798\n  21  0.8158587  0.6299744\n  22  0.8153959  0.6288475\n  23  0.8153959  0.6287809\n  24  0.8135814  0.6251348\n  25  0.8149453  0.6276014\n  26  0.8131623  0.6243492\n  27  0.8118170  0.6214666\n  28  0.8068248  0.6113005\n  29  0.8081823  0.6139508\n  30  0.8049981  0.6073241\n  31  0.8086576  0.6145954\n  32  0.8091083  0.6153964\n  33  0.8095899  0.6164358\n  34  0.8086516  0.6142100\n  35  0.8077320  0.6122174\n  36  0.8086516  0.6139925\n  37  0.8073193  0.6110468\n  38  0.8104783  0.6173673\n  39  0.8095774  0.6154771\n  40  0.8127741  0.6213013\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 20.\n\n# Make predictions\nknn_predictions &lt;- predict(knn_model, newdata = heart_test)\n\n# Step 11: Evaluate the model\nconf_matrix &lt;- confusionMatrix(knn_predictions, heart_test$HeartDiseaseFactor)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 73 18\n         1  9 83\n                                          \n               Accuracy : 0.8525          \n                 95% CI : (0.7926, 0.9005)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.7048          \n                                          \n Mcnemar's Test P-Value : 0.1237          \n                                          \n            Sensitivity : 0.8902          \n            Specificity : 0.8218          \n         Pos Pred Value : 0.8022          \n         Neg Pred Value : 0.9022          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3989          \n   Detection Prevalence : 0.4973          \n      Balanced Accuracy : 0.8560          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework 5.html#logistic-regression",
    "href": "Homework 5.html#logistic-regression",
    "title": "Homework 5",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# Define the models\n# Define logistic regression models\nmodel1_formula &lt;- HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR\nmodel2_formula &lt;- HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR + factor(SexM) + factor(ExerciseAnginaN) + factor(ChestPainTypeASY) + factor(RestingECGNormal)\nmodel3_formula &lt;- HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR + I(Age^2) + RestingBP:Cholesterol\n\nmodel1_train &lt;- train(\n  model1_formula,\n  heart_train, \n  method = \"knn\",\n  trControl = train_control,\n  tuneGrid = tune_grid,\n  preProcess = c(\"center\", \"scale\")\n)\n\nmodel2_train &lt;- train(\n  model2_formula,\n  heart_train, \n  method = \"knn\",\n  trControl = train_control,\n  tuneGrid = tune_grid,\n  preProcess = c(\"center\", \"scale\")\n)\n\nmodel3_train &lt;- train(\n  model3_formula,\n  heart_train, \n  method = \"knn\",\n  trControl = train_control,\n  tuneGrid = tune_grid,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Extract resamples into a list\nresults &lt;- resamples(list(model1 = model1_train, model2 = model2_train, model3 = model3_train))\nprint(results)\n\n\nCall:\nresamples.default(x = list(model1 = model1_train, model2 = model2_train,\n model3 = model3_train))\n\nModels: model1, model2, model3 \nNumber of resamples: 30 \nPerformance metrics: Accuracy, Kappa \nTime estimates for: everything, final model fit \n\n# Summarize the results\nresults &lt;- summary(results)\nprint(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: model1, model2, model3 \nNumber of resamples: 30 \n\nAccuracy \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.6081081 0.6996483 0.7297297 0.7248704 0.7432432 0.8194444    0\nmodel2 0.6986301 0.7729082 0.8082192 0.8013681 0.8372825 0.8767123    0\nmodel3 0.6164384 0.6949741 0.7329204 0.7316246 0.7808219 0.8219178    0\n\nKappa \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.1852696 0.3990788 0.4465221 0.4402578 0.4811774 0.6261981    0\nmodel2 0.3921272 0.5402739 0.6106666 0.5980343 0.6690523 0.7504747    0\nmodel3 0.2378822 0.3802680 0.4568175 0.4529283 0.5517964 0.6433672    0\n\n# Extract accuracies\nmodel_accuracies &lt;- results$statistics$Accuracy[, \"Mean\"]\n\n# Identify the best model\nbest_model_name &lt;- names(which.max(model_accuracies))\nprint(paste(\"Best model:\", best_model_name))\n\n[1] \"Best model: model2\"\n\n# Retrieve the best model\nbest_model_train &lt;- get(paste0(best_model_name, \"_train\"))\nprint(best_model_train)\n\nk-Nearest Neighbors \n\n735 samples\n  8 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 663, 661, 662, 661, 661, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7116099  0.4190119\n   2  0.7224889  0.4389113\n   3  0.7664606  0.5263316\n   4  0.7700702  0.5344305\n   5  0.7769012  0.5489382\n   6  0.7805607  0.5567951\n   7  0.7832511  0.5627088\n   8  0.7886935  0.5735801\n   9  0.7914085  0.5787278\n  10  0.7864104  0.5690396\n  11  0.7846019  0.5650629\n  12  0.7850403  0.5659686\n  13  0.7900508  0.5758464\n  14  0.7864287  0.5684511\n  15  0.7914330  0.5784763\n  16  0.7968326  0.5894039\n  17  0.7959627  0.5877875\n  18  0.7932724  0.5826248\n  19  0.7918902  0.5796149\n  20  0.7932417  0.5823602\n  21  0.7941424  0.5843699\n  22  0.7964194  0.5887777\n  23  0.7991529  0.5941488\n  24  0.7927847  0.5810615\n  25  0.7973388  0.5904962\n  26  0.7987025  0.5932098\n  27  0.7977831  0.5913963\n  28  0.7968947  0.5894457\n  29  0.8000478  0.5953599\n  30  0.7995974  0.5945501\n  31  0.7977769  0.5909405\n  32  0.7950372  0.5853643\n  33  0.7991219  0.5937497\n  34  0.7991096  0.5933258\n  35  0.7991221  0.5936421\n  36  0.7986654  0.5925527\n  37  0.8009300  0.5974096\n  38  0.8013681  0.5980343\n  39  0.8000168  0.5954851\n  40  0.8000228  0.5952538\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 38.\n\n# Evaluate the best model on the test set\npredictions &lt;- predict(best_model_train, newdata = heart_test)\nconf_matrix &lt;- confusionMatrix(predictions, heart_test$HeartDiseaseFactor)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 71 17\n         1 11 84\n                                          \n               Accuracy : 0.847           \n                 95% CI : (0.7865, 0.8959)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6928          \n                                          \n Mcnemar's Test P-Value : 0.3447          \n                                          \n            Sensitivity : 0.8659          \n            Specificity : 0.8317          \n         Pos Pred Value : 0.8068          \n         Neg Pred Value : 0.8842          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3880          \n   Detection Prevalence : 0.4809          \n      Balanced Accuracy : 0.8488          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework 5.html#tree-models",
    "href": "Homework 5.html#tree-models",
    "title": "Homework 5",
    "section": "Tree Models",
    "text": "Tree Models"
  },
  {
    "objectID": "Homework 5.html#decision-tree-model",
    "href": "Homework 5.html#decision-tree-model",
    "title": "Homework 5",
    "section": "Decision Tree Model",
    "text": "Decision Tree Model\n\nlibrary(caret)\n\n# Define the formula for the decision tree model\ntree_formula &lt;- HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR + factor(SexM) + factor(ExerciseAnginaN) + factor(ChestPainTypeASY) + factor(RestingECGNormal)\n\n# Define tuning grid for rpart (cp values)\ncp_values &lt;- seq(0, 0.1, by = 0.001)\ntune_grid2 &lt;- expand.grid(cp = cp_values)\n\n# Train the decision tree model\nset.seed(1234)\nmodel_rpart_train &lt;- train(\n  tree_formula,\n  data = heart_train,\n  method = \"rpart\",\n  trControl = train_control,\n  tuneGrid = tune_grid2,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Print the best model and its parameters\nprint(model_rpart_train)\n\nCART \n\n735 samples\n  8 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 662, 662, 661, 661, 663, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.8009246  0.5954664\n  0.001  0.8009246  0.5954664\n  0.002  0.8059165  0.6049195\n  0.003  0.8059165  0.6049195\n  0.004  0.8090447  0.6112299\n  0.005  0.8104022  0.6141802\n  0.006  0.8108465  0.6151931\n  0.007  0.8117347  0.6165070\n  0.008  0.8117347  0.6165945\n  0.009  0.8135363  0.6197105\n  0.010  0.8099202  0.6122949\n  0.011  0.8067614  0.6051412\n  0.012  0.8031206  0.5967929\n  0.013  0.8004487  0.5897828\n  0.014  0.7963451  0.5814442\n  0.015  0.7967894  0.5820790\n  0.016  0.7935737  0.5752198\n  0.017  0.7885941  0.5657168\n  0.018  0.7885941  0.5657168\n  0.019  0.7867923  0.5622482\n  0.020  0.7867923  0.5622482\n  0.021  0.7822567  0.5538842\n  0.022  0.7813435  0.5522078\n  0.023  0.7786408  0.5471586\n  0.024  0.7781903  0.5464853\n  0.025  0.7781903  0.5464853\n  0.026  0.7781903  0.5465757\n  0.027  0.7781903  0.5465757\n  0.028  0.7763885  0.5427070\n  0.029  0.7763885  0.5427070\n  0.030  0.7763885  0.5427070\n  0.031  0.7750372  0.5402989\n  0.032  0.7750372  0.5402989\n  0.033  0.7750372  0.5402989\n  0.034  0.7714089  0.5324742\n  0.035  0.7714089  0.5324742\n  0.036  0.7714089  0.5324742\n  0.037  0.7714089  0.5324742\n  0.038  0.7650779  0.5208262\n  0.039  0.7650779  0.5208262\n  0.040  0.7650779  0.5208262\n  0.041  0.7641645  0.5194010\n  0.042  0.7641645  0.5194010\n  0.043  0.7641645  0.5194010\n  0.044  0.7641645  0.5194010\n  0.045  0.7641645  0.5195907\n  0.046  0.7641645  0.5195907\n  0.047  0.7641645  0.5195907\n  0.048  0.7637017  0.5191498\n  0.049  0.7637017  0.5191498\n  0.050  0.7637017  0.5191498\n  0.051  0.7618749  0.5186928\n  0.052  0.7618749  0.5186928\n  0.053  0.7618749  0.5186928\n  0.054  0.7618749  0.5186928\n  0.055  0.7618749  0.5186928\n  0.056  0.7618749  0.5186928\n  0.057  0.7618749  0.5186928\n  0.058  0.7618749  0.5189584\n  0.059  0.7618749  0.5189584\n  0.060  0.7618749  0.5189584\n  0.061  0.7618749  0.5189584\n  0.062  0.7623253  0.5199274\n  0.063  0.7623253  0.5199274\n  0.064  0.7623253  0.5199274\n  0.065  0.7632386  0.5219487\n  0.066  0.7632386  0.5219487\n  0.067  0.7632386  0.5219487\n  0.068  0.7632386  0.5219487\n  0.069  0.7632386  0.5219487\n  0.070  0.7632386  0.5219487\n  0.071  0.7632386  0.5219487\n  0.072  0.7632386  0.5219487\n  0.073  0.7632386  0.5219487\n  0.074  0.7632386  0.5219487\n  0.075  0.7632386  0.5219487\n  0.076  0.7632386  0.5219487\n  0.077  0.7632386  0.5219487\n  0.078  0.7632386  0.5219487\n  0.079  0.7632386  0.5219487\n  0.080  0.7632386  0.5219487\n  0.081  0.7632386  0.5219487\n  0.082  0.7632386  0.5219487\n  0.083  0.7632386  0.5219487\n  0.084  0.7632386  0.5219487\n  0.085  0.7632386  0.5219487\n  0.086  0.7632386  0.5219487\n  0.087  0.7632386  0.5219487\n  0.088  0.7632386  0.5219487\n  0.089  0.7632386  0.5219487\n  0.090  0.7632386  0.5219487\n  0.091  0.7632386  0.5219487\n  0.092  0.7632386  0.5219487\n  0.093  0.7632386  0.5219487\n  0.094  0.7632386  0.5219487\n  0.095  0.7632386  0.5219487\n  0.096  0.7632386  0.5219487\n  0.097  0.7632386  0.5219487\n  0.098  0.7632386  0.5219487\n  0.099  0.7632386  0.5219487\n  0.100  0.7632386  0.5219487\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.009.\n\n# Evaluate the decision tree model on the test set\npredictions_rpart &lt;- predict(model_rpart_train, newdata = heart_test)\nconf_matrix_rpart &lt;- confusionMatrix(predictions_rpart, heart_test$HeartDiseaseFactor)\nprint(conf_matrix_rpart)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 66 15\n         1 16 86\n                                          \n               Accuracy : 0.8306          \n                 95% CI : (0.7683, 0.8819)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : 1.349e-15       \n                                          \n                  Kappa : 0.6571          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8049          \n            Specificity : 0.8515          \n         Pos Pred Value : 0.8148          \n         Neg Pred Value : 0.8431          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3607          \n   Detection Prevalence : 0.4426          \n      Balanced Accuracy : 0.8282          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n##Rain Forest Model\n\n# Set up empty var for tuning.\nnum_predictors &lt;- length(all.vars(tree_formula)) - 1  \n\n# New Tuning for this tree fit\ntuneGrid &lt;- expand.grid(mtry = 1:num_predictors)\n\nset.seed(1234)\nmodel_rf_train &lt;- train(\n  tree_formula,\n  data = heart_train,\n  method = \"rf\",\n  trControl = train_control,\n  tuneGrid = tuneGrid,\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Print the best model and its parameters\nprint(model_rf_train)\n\nRandom Forest \n\n735 samples\n  8 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 662, 662, 661, 661, 663, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.8091132  0.6109968\n  2     0.8122540  0.6182520\n  3     0.8005051  0.5942969\n  4     0.7955320  0.5849629\n  5     0.7932484  0.5802677\n  6     0.7877934  0.5696301\n  7     0.7873678  0.5688249\n  8     0.7864793  0.5674620\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n# Evaluate the decision tree model on the test set\npredictions_rpart &lt;- predict(model_rf_train, newdata = heart_test)\nconf_matrix_rpart &lt;- confusionMatrix(predictions_rpart, heart_test$HeartDiseaseFactor)\nprint(conf_matrix_rpart)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 66 16\n         1 16 85\n                                          \n               Accuracy : 0.8251          \n                 95% CI : (0.7622, 0.8772)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : 5.267e-15       \n                                          \n                  Kappa : 0.6465          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8049          \n            Specificity : 0.8416          \n         Pos Pred Value : 0.8049          \n         Neg Pred Value : 0.8416          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3607          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8232          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n##Booted Tree Model\n\n# Define the tuning grid for the boosted tree\ntuneGrid2 &lt;- expand.grid(\n  n.trees = c(25, 50, 100, 200),\n  interaction.depth = c(1, 2, 3),\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n\n# Train the boosted tree model using caret\nset.seed(1234)\nmodel_gbm_train &lt;- train(\n  tree_formula,\n  data = heart_train,\n  method = \"gbm\",\n  trControl = train_control,\n  tuneGrid = tuneGrid2,\n  preProcess = c(\"center\", \"scale\"),\n  verbose = FALSE\n)\n\n# Print the best model and its parameters\nprint(model_gbm_train)\n\nStochastic Gradient Boosting \n\n735 samples\n  8 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 662, 662, 661, 661, 663, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7904648  0.5725580\n  1                   50      0.7973647  0.5882098\n  1                  100      0.7982532  0.5910504\n  1                  200      0.8059417  0.6064536\n  2                   25      0.7959010  0.5834330\n  2                   50      0.8009115  0.5949879\n  2                  100      0.8023378  0.5981493\n  2                  200      0.7996478  0.5927901\n  3                   25      0.8077058  0.6079118\n  3                   50      0.8073178  0.6083771\n  3                  100      0.8086566  0.6110532\n  3                  200      0.8041274  0.6020144\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 100, interaction.depth =\n 3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n##Evaluate\n\n# Evaluate on test set and print confusion matrices\n# Decision Tree (rpart)\npredictions_rpart &lt;- predict(model_rpart_train, newdata = heart_test)\nconf_matrix_rpart &lt;- confusionMatrix(predictions_rpart, heart_test$HeartDiseaseFactor)\nprint(\"Confusion Matrix for Decision Tree (rpart):\")\n\n[1] \"Confusion Matrix for Decision Tree (rpart):\"\n\nprint(conf_matrix_rpart)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 66 15\n         1 16 86\n                                          \n               Accuracy : 0.8306          \n                 95% CI : (0.7683, 0.8819)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : 1.349e-15       \n                                          \n                  Kappa : 0.6571          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8049          \n            Specificity : 0.8515          \n         Pos Pred Value : 0.8148          \n         Neg Pred Value : 0.8431          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3607          \n   Detection Prevalence : 0.4426          \n      Balanced Accuracy : 0.8282          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# Random Forest (rf)\npredictions_rf &lt;- predict(model_rf_train, newdata = heart_test)\nconf_matrix_rf &lt;- confusionMatrix(predictions_rf, heart_test$HeartDiseaseFactor)\nprint(\"Confusion Matrix for Random Forest (rf):\")\n\n[1] \"Confusion Matrix for Random Forest (rf):\"\n\nprint(conf_matrix_rf)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 66 16\n         1 16 85\n                                          \n               Accuracy : 0.8251          \n                 95% CI : (0.7622, 0.8772)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : 5.267e-15       \n                                          \n                  Kappa : 0.6465          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8049          \n            Specificity : 0.8416          \n         Pos Pred Value : 0.8049          \n         Neg Pred Value : 0.8416          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3607          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8232          \n                                          \n       'Positive' Class : 0               \n                                          \n\n# Boosted Tree (gbm)\npredictions_gbm &lt;- predict(model_gbm_train, newdata = heart_test)\nconf_matrix_gbm &lt;- confusionMatrix(predictions_gbm, heart_test$HeartDiseaseFactor)\nprint(\"Confusion Matrix for Boosted Tree (gbm):\")\n\n[1] \"Confusion Matrix for Boosted Tree (gbm):\"\n\nprint(conf_matrix_gbm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 69 14\n         1 13 87\n                                          \n               Accuracy : 0.8525          \n                 95% CI : (0.7926, 0.9005)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.702           \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8415          \n            Specificity : 0.8614          \n         Pos Pred Value : 0.8313          \n         Neg Pred Value : 0.8700          \n             Prevalence : 0.4481          \n         Detection Rate : 0.3770          \n   Detection Prevalence : 0.4536          \n      Balanced Accuracy : 0.8514          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n##Wrap Up The model with the highest accuracy value indicates which model’s predictions aligned the closest with the actual outcomes in the test data. Amongst the three models, the one with the highest accuracy was the boosted tree model."
  }
]